{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0L7D4M1IXsj",
        "outputId": "730a44df-e1f5-4748-8c4d-d2da2e2a9279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ1F0ULoIlZA",
        "outputId": "ed2bc21e-9791-4ac3-8e11-eab85d28c88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.18.0+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation_models_pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.3.0+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.23.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=8fbc5a5261072ac0db1a5da1dd162bb7f930381688e1a50617fd8b2069077077\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=9950866efdfff8e7edaecc93f095b0b171d40344958c23e2db2e17fff157b8f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AqDrI5HIDlb"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ0YziqQInaa",
        "outputId": "40e16387-b193-4cd6-8db1-1af0b9830d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = smp.DeepLabV3Plus(encoder_name='resnet101', classes=6, activation='softmax').to(device)\n",
        "model = smp.UnetPlusPlus(encoder_name='resnet101', classes=6, activation='softmax').to(device)"
      ],
      "metadata": {
        "id": "pw_S0hrBIram"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_size = (512, 512)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(resize_size),  # 이미지 크기 조정\n",
        "    transforms.ToTensor(),           # 이미지를 텐서로 변환\n",
        "])\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize(resize_size),\n",
        "    transforms.Lambda(lambda x: torch.tensor(np.array(x)))\n",
        "])\n"
      ],
      "metadata": {
        "id": "B5yKnToFItwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test data"
      ],
      "metadata": {
        "id": "CsgvDmJjI0uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anger\n",
        "model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='./test/1. anger'\n",
        "result_path='./seg_result_deeplab'\n",
        "image_files = os.listdir(input_path)\n",
        "# image_files = sorted(os.listdir(input_path))\n",
        "segmentation_maps_dict = {}\n",
        "for image_name in tqdm(image_files):\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    input = cv2.imread(image_path)\n",
        "    input_shape = input.shape\n",
        "    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "    input = Image.fromarray(input)\n",
        "    input = transform(input)\n",
        "    input=input.unsqueeze(0)\n",
        "    input = input.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input)\n",
        "    segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "    resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # NumPy 배열로 변환\n",
        "    resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "    segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "np.savez(os.path.join(result_path, 'seg_test_anger.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "id": "u56c5E1dIut3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "0ccda3cc-46b6-4985-f3d6-322ce96f632c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './test/1. anger'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-434937c37e76>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./test/1. anger'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./seg_result_deeplab'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# image_files = sorted(os.listdir(input_path))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msegmentation_maps_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './test/1. anger'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#happy\n",
        "model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='./test/2. happy'\n",
        "result_path='./seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "segmentation_maps_dict = {}\n",
        "for image_name in tqdm(image_files):\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    input = cv2.imread(image_path)\n",
        "    input_shape = input.shape\n",
        "    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "    input = Image.fromarray(input)\n",
        "    input = transform(input)\n",
        "    input=input.unsqueeze(0)\n",
        "    input = input.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input)\n",
        "    segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "    resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # NumPy 배열로 변환\n",
        "    resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "    segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "np.savez(os.path.join(result_path, 'seg_test_happy.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "id": "89x_3yWbIvyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#panic\n",
        "model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='./test/3. panic'\n",
        "result_path='./seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "segmentation_maps_dict = {}\n",
        "for image_name in tqdm(image_files):\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    input = cv2.imread(image_path)\n",
        "    input_shape = input.shape\n",
        "    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "    input = Image.fromarray(input)\n",
        "    input = transform(input)\n",
        "    input=input.unsqueeze(0)\n",
        "    input = input.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input)\n",
        "    segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "    resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # NumPy 배열로 변환\n",
        "    resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "    segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "np.savez(os.path.join(result_path, 'seg_test_panic.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "id": "ovmdbLLPIxPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sadness\n",
        "model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='./test/4. sadness'\n",
        "result_path='./seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "segmentation_maps_dict = {}\n",
        "for image_name in tqdm(image_files):\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    input = cv2.imread(image_path)\n",
        "    input_shape = input.shape\n",
        "    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "    input = Image.fromarray(input)\n",
        "    input = transform(input)\n",
        "    input=input.unsqueeze(0)\n",
        "    input = input.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input)\n",
        "    segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "    resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # NumPy 배열로 변환\n",
        "    resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "    segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "# np.savez(os.path.join(result_path, 'seg_test_sadness.npz'), **segmentation_maps_dict)\n",
        "np.savez(os.path.join(result_path, 'seg_test_unet_sadness.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "id": "TPsKR8fmIx3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aug data"
      ],
      "metadata": {
        "id": "j79TyYMfI2dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "T8Fl6NbRpbfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_anger_zip = '/content/drive/MyDrive/aug/anger-aug.zip'\n",
        "extract_to_directory = 'aug_anger'\n",
        "if not os.path.exists(extract_to_directory):\n",
        "    os.makedirs(extract_to_directory)\n",
        "with zipfile.ZipFile(aug_anger_zip, 'r') as zip_ref:\n",
        "    # 모든 파일 압축 해제\n",
        "    zip_ref.extractall(extract_to_directory)\n",
        "    print(\"압축 해제가 완료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xJ1U6BHpA5u",
        "outputId": "f21ff353-764f-4398-fc09-c6b9351fc908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제가 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aug_anger\n",
        "# model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model_path='/content/drive/MyDrive/seg/Unet++_model_epoch8.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='/content/aug_anger/anger'\n",
        "result_path='/content/drive/MyDrive/seg/seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "\n",
        "batch_size = 1500\n",
        "for i in range(len(image_files) // batch_size + 1):\n",
        "  segmentation_maps_dict = {}\n",
        "  start_idx = i * batch_size\n",
        "  end_idx = min((i + 1) * batch_size, len(image_files))\n",
        "  image_files_sep = image_files[start_idx:end_idx]\n",
        "  for image_name in tqdm(image_files_sep):\n",
        "      image_path = os.path.join(input_path, image_name)\n",
        "      input = cv2.imread(image_path)\n",
        "      input_shape = input.shape\n",
        "      input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "      input = Image.fromarray(input)\n",
        "      input = transform(input)\n",
        "      input=input.unsqueeze(0)\n",
        "      input = input.to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input)\n",
        "      segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "      resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "      # NumPy 배열로 변환\n",
        "      resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "      segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "  # np.savez_compressed(os.path.join(result_path, f'seg_aug_anger{i+1}.npz'), **segmentation_maps_dict)\n",
        "  np.savez_compressed(os.path.join(result_path, f'seg_unet_aug_anger{i+1}.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhAXYb6KI4ez",
        "outputId": "95e9167b-58f5-48ea-af98-38044658d73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [05:53<00:00,  4.24it/s]\n",
            "100%|██████████| 1500/1500 [05:55<00:00,  4.21it/s]\n",
            "100%|██████████| 1500/1500 [05:53<00:00,  4.25it/s]\n",
            "100%|██████████| 212/212 [00:48<00:00,  4.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_happy_zip = '/content/drive/MyDrive/aug/happy-aug.zip'\n",
        "extract_to_directory = 'aug_happy'\n",
        "if not os.path.exists(extract_to_directory):\n",
        "    os.makedirs(extract_to_directory)\n",
        "with zipfile.ZipFile(aug_happy_zip, 'r') as zip_ref:\n",
        "    # 모든 파일 압축 해제\n",
        "    zip_ref.extractall(extract_to_directory)\n",
        "    print(\"압축 해제가 완료되었습니다.\")"
      ],
      "metadata": {
        "id": "tgulLpqjJmZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed391bd3-9e2b-4c0d-e6a0-673ed43ef14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제가 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -r aug_happy"
      ],
      "metadata": {
        "id": "5UmGql3kIJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#aug_happy\n",
        "# model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model_path='/content/drive/MyDrive/seg/Unet++_model_epoch8.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='/content/aug_happy/happy'\n",
        "result_path='/content/drive/MyDrive/seg/seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "\n",
        "batch_size = 1500\n",
        "for i in range(len(image_files) // batch_size + 1):\n",
        "  segmentation_maps_dict = {}\n",
        "  start_idx = i * batch_size\n",
        "  end_idx = min((i + 1) * batch_size, len(image_files))\n",
        "  image_files_sep = image_files[start_idx:end_idx]\n",
        "  for image_name in tqdm(image_files_sep):\n",
        "      image_path = os.path.join(input_path, image_name)\n",
        "      input = cv2.imread(image_path)\n",
        "      input_shape = input.shape\n",
        "      input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "      input = Image.fromarray(input)\n",
        "      input = transform(input)\n",
        "      input=input.unsqueeze(0)\n",
        "      input = input.to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input)\n",
        "      segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "      resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "      # NumPy 배열로 변환\n",
        "      resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "      segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "  # np.savez_compressed(os.path.join(result_path, f'seg_aug_happy{i+1}.npz'), **segmentation_maps_dict)\n",
        "  np.savez_compressed(os.path.join(result_path, f'seg_unet_aug_happy{i+1}.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVk2blK6G2TH",
        "outputId": "4eaecfa7-cd7a-4baa-b8fc-68aa626e5b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [05:54<00:00,  4.23it/s]\n",
            "100%|██████████| 1500/1500 [05:55<00:00,  4.22it/s]\n",
            "100%|██████████| 1500/1500 [05:52<00:00,  4.26it/s]\n",
            "100%|██████████| 479/479 [01:54<00:00,  4.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_panic_zip = '/content/drive/MyDrive/aug/panic-aug.zip'\n",
        "extract_to_directory = 'aug_panic'\n",
        "if not os.path.exists(extract_to_directory):\n",
        "    os.makedirs(extract_to_directory)\n",
        "with zipfile.ZipFile(aug_panic_zip, 'r') as zip_ref:\n",
        "    # 모든 파일 압축 해제\n",
        "    zip_ref.extractall(extract_to_directory)\n",
        "    print(\"압축 해제가 완료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PmsCZacMhe_",
        "outputId": "157d13ed-89c8-4d6e-c1a8-5c483e104974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제가 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aug_panic\n",
        "model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "# model_path='/content/drive/MyDrive/seg/Unet++_model_epoch8.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='/content/aug_panic/panic'\n",
        "result_path='/content/drive/MyDrive/seg/seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "\n",
        "batch_size = 1500\n",
        "for i in range(len(image_files) // batch_size + 1):\n",
        "  segmentation_maps_dict = {}\n",
        "  start_idx = i * batch_size\n",
        "  end_idx = min((i + 1) * batch_size, len(image_files))\n",
        "  image_files_sep = image_files[start_idx:end_idx]\n",
        "  for image_name in tqdm(image_files_sep):\n",
        "      image_path = os.path.join(input_path, image_name)\n",
        "      input = cv2.imread(image_path)\n",
        "      input_shape = input.shape\n",
        "      input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "      input = Image.fromarray(input)\n",
        "      input = transform(input)\n",
        "      input=input.unsqueeze(0)\n",
        "      input = input.to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input)\n",
        "      segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "      resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "      # NumPy 배열로 변환\n",
        "      resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "      segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "  np.savez_compressed(os.path.join(result_path, f'seg_aug_panic{i+1}.npz'), **segmentation_maps_dict)\n",
        "  # np.savez_compressed(os.path.join(result_path, f'seg_unet_aug_panic{i+1}.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "id": "Y62vvZH6XgIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_sadness_zip = '/content/drive/MyDrive/aug/sadness-aug.zip'\n",
        "extract_to_directory = 'aug_sadness'\n",
        "if not os.path.exists(extract_to_directory):\n",
        "    os.makedirs(extract_to_directory)\n",
        "with zipfile.ZipFile(aug_sadness_zip, 'r') as zip_ref:\n",
        "    # 모든 파일 압축 해제\n",
        "    zip_ref.extractall(extract_to_directory)\n",
        "    print(\"압축 해제가 완료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c_8fzM7Y6jF",
        "outputId": "9238b144-8269-4102-855f-0d304044c970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제가 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aug_sadness\n",
        "# model_path='/content/drive/MyDrive/seg/deeplabv3plus_model_epoch9_cd.pth'\n",
        "model_path='/content/drive/MyDrive/seg/Unet++_model_epoch8.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "input_path='/content/aug_sadness/sadness'\n",
        "result_path='/content/drive/MyDrive/seg/seg_result_deeplab'\n",
        "# image_files = os.listdir(input_path)\n",
        "image_files = sorted(os.listdir(input_path))\n",
        "\n",
        "batch_size = 1500\n",
        "for i in range(len(image_files) // batch_size + 1):\n",
        "  segmentation_maps_dict = {}\n",
        "  start_idx = i * batch_size\n",
        "  end_idx = min((i + 1) * batch_size, len(image_files))\n",
        "  image_files_sep = image_files[start_idx:end_idx]\n",
        "  for image_name in tqdm(image_files_sep):\n",
        "      image_path = os.path.join(input_path, image_name)\n",
        "      input = cv2.imread(image_path)\n",
        "      input_shape = input.shape\n",
        "      input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
        "      input = Image.fromarray(input)\n",
        "      input = transform(input)\n",
        "      input=input.unsqueeze(0)\n",
        "      input = input.to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input)\n",
        "      segmentation_map = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "      resized_segmentation_map = cv2.resize(segmentation_map, (input_shape[1], input_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "      # NumPy 배열로 변환\n",
        "      resized_segmentation_map_np = np.array(resized_segmentation_map)\n",
        "\n",
        "      segmentation_maps_dict[image_name] = resized_segmentation_map\n",
        "\n",
        "  # np.savez_compressed(os.path.join(result_path, f'seg_aug_sadness{i+1}.npz'), **segmentation_maps_dict)\n",
        "  np.savez_compressed(os.path.join(result_path, f'seg_unet_aug_sadness{i+1}.npz'), **segmentation_maps_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hapoesdnZTyR",
        "outputId": "df21d3e2-def6-470a-d15c-e1cde1e8b897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [05:52<00:00,  4.26it/s]\n",
            "100%|██████████| 1500/1500 [05:54<00:00,  4.23it/s]\n",
            "100%|██████████| 1500/1500 [05:50<00:00,  4.28it/s]\n",
            "100%|██████████| 623/623 [02:25<00:00,  4.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hOcWraDl3snA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}